{"cells":[{"cell_type":"markdown","metadata":{"id":"Waaq0V2Ihw4G"},"source":["##Importing Dataset\n","\n","In this section I import the relevant libraries for loading, cleaning and proccessing the dataset that I will use for the baseline testing. I also loaded the dataset in for google drive and perform any cleaning and proccessign tasks that need to be done."]},{"cell_type":"code","execution_count":72,"metadata":{"executionInfo":{"elapsed":711,"status":"ok","timestamp":1702918231498,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"87Dzv5kmhdWw"},"outputs":[],"source":["import pandas as pd\n","import string"]},{"cell_type":"code","execution_count":73,"metadata":{"executionInfo":{"elapsed":22795,"status":"ok","timestamp":1702918277128,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"Bd-wYpTNhpzT"},"outputs":[],"source":["df = pd.read_table('../../data/etymwn.tsv', header=None)"]},{"cell_type":"markdown","metadata":{"id":"cDsb4XHqi7sf"},"source":["The dataset I have choosen is a etymological wordnet dataset form http://etym.org/\n","\n","As you can see below, the dataset consists of three colummns. Two columns contain a word and its ISO-639-3 language code and the other column contains the etymological relationship between them. Some cleaning is required to get thihs data into a usable format"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1702918277128,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"c5DsvquIi455","outputId":"57cffcc1-015f-43e3-9175-64a5306603b1"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>aaq: Pawanobskewi</td>\n","      <td>rel:etymological_origin_of</td>\n","      <td>eng: Penobscot</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>aaq: senabe</td>\n","      <td>rel:etymological_origin_of</td>\n","      <td>eng: sannup</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>abe: waniigan</td>\n","      <td>rel:etymological_origin_of</td>\n","      <td>eng: wangan</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>abe: waniigan</td>\n","      <td>rel:etymological_origin_of</td>\n","      <td>eng: wannigan</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>abs: beta</td>\n","      <td>rel:etymological_origin_of</td>\n","      <td>zsm: beta</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   0                           1               2\n","0  aaq: Pawanobskewi  rel:etymological_origin_of  eng: Penobscot\n","1        aaq: senabe  rel:etymological_origin_of     eng: sannup\n","2      abe: waniigan  rel:etymological_origin_of     eng: wangan\n","3      abe: waniigan  rel:etymological_origin_of   eng: wannigan\n","4          abs: beta  rel:etymological_origin_of       zsm: beta"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":75,"metadata":{"executionInfo":{"elapsed":4688,"status":"ok","timestamp":1702918281812,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"Yl_n_S2zipRG"},"outputs":[],"source":["df2 = df.loc[df[0].str[:3] == 'eng']"]},{"cell_type":"code","execution_count":76,"metadata":{"executionInfo":{"elapsed":234,"status":"ok","timestamp":1702918282045,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"U8bZrwT4iq1h"},"outputs":[],"source":["df3 = df2.loc[df2[1].str[4:] == 'etymology']"]},{"cell_type":"code","execution_count":77,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702918282046,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"ul1qFjyjizBy","outputId":"d0b8c36c-e679-4040-9b3d-8cdef8607f3f"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\stang\\AppData\\Local\\Temp\\ipykernel_9236\\2486685936.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df4[2] = df4[2].str[:3]\n","C:\\Users\\stang\\AppData\\Local\\Temp\\ipykernel_9236\\2486685936.py:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df4[0] = df4[0].str[4:]\n"]}],"source":["df4 = df3.loc[df3[2].str[:3] != 'eng']\n","\n","df4[2] = df4[2].str[:3]\n","\n","df4[0] = df4[0].str[4:]"]},{"cell_type":"code","execution_count":78,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702918282046,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"kCpNzeTprL_0"},"outputs":[],"source":["df4 = df4.drop(1, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"9AsN0jMXiuWv"},"source":["## Language Codes\n","\n","I will now import a table that contains the language associated with each ISO-639-3 language code. I will do this so I can replace the language codes with the name of the language to make the dataset more interpretable"]},{"cell_type":"code","execution_count":79,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702918282046,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"8kNS0BTukWVV"},"outputs":[],"source":["lanAlt = pd.read_csv('../../data/iso-639-3_Name_Index_20230123.tab', sep='\\t')"]},{"cell_type":"code","execution_count":80,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702918282046,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"x9jQ1-AakbEd"},"outputs":[],"source":["#Some language codes in the data set were not present in the language code file\n","#Because of this I added them manually\n","lang_dict = dict(zip(lanAlt['Id'], lanAlt['Print_Name']))\n","lang_dict['p_s'] = 'unknown'\n","lang_dict['nah'] = 'Nahuatl'\n","lang_dict['nan'] = 'Min Nan'\n","lang_dict['wit'] = 'unknown'"]},{"cell_type":"code","execution_count":81,"metadata":{"executionInfo":{"elapsed":1948,"status":"ok","timestamp":1702918283989,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"scnJoXDgkqpO"},"outputs":[],"source":["for i, v in df4.iterrows():\n","    df4[2][i] = lang_dict[v[2]]\n","    v[0] = v[0].replace('-', '')\n","    v[0] = v[0].replace(\"'\", \"\")"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702918283989,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"7Jk9CIwjkz8h","outputId":"db85d731-2849-491c-a9cc-d1f4426a6fbf"},"outputs":[],"source":["df4[2] = df4[2].str.replace(r'\\(.*\\)', '', regex=True)"]},{"cell_type":"markdown","metadata":{"id":"l_KpR6ZnlOQx"},"source":["There are many languages present in the current dataset and to build any model with that many classes to predict would result in an extremely skewed model as most classes would be under-represented. To make things slightly easier, I will take the top ten largest classes and build a model around them."]},{"cell_type":"code","execution_count":85,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702918286038,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"O-kGWwW2lIOy"},"outputs":[],"source":["language_counts = df4[2].value_counts()\n","result_df = pd.DataFrame({'Language': language_counts.index, 'Count': language_counts.values})\n","topLang = result_df['Language'].tolist()[:10]"]},{"cell_type":"code","execution_count":86,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702918286038,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"_1SHSkUQnBH-","outputId":"a305a309-3402-4e15-d8fd-ccdbfa55be82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Amount of languages in the dataset: 238\n"]}],"source":["print(\"Amount of languages in the dataset: \" + str(len(result_df['Language'].tolist())))"]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702918286038,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"AzkTh-UioHJo","outputId":"0327080c-4c2f-4357-89b5-71bc23a723cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top ten languages: \n","Latin\n","Middle English \n","French\n","Ancient Greek \n","Old French \n","Old English \n","Middle French \n","Japanese\n","Italian\n","Spanish\n"]}],"source":["print(\"Top ten languages: \")\n","for i in topLang:\n","  print(i)"]},{"cell_type":"code","execution_count":88,"metadata":{"executionInfo":{"elapsed":8021,"status":"ok","timestamp":1702918294057,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"QiEdFz0NmYg9"},"outputs":[],"source":["drop_in = []\n","\n","#getting the index of languages that aren't in the top ten\n","for i, v in df4.iterrows():\n","  if v[2] not in topLang:\n","    drop_in.append(i)\n","\n","#dropping these languages from the data\n","for i in drop_in:\n","  df4.drop(i, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"Ytv8tEkrmree"},"source":["## Baseline Test\n","\n","The idea behind this baseline test is that different languages will have ngrams of characters that are somewhat unique to that language. Of course this is not always the case and languages that are similar to each other such as the romantic lanuages will often share many of theses ngrams.\n","\n","The ngram character combinations were determinded through a mix of my own prior knowledge of some languages, trail and error as well as asking ChatGPT. The prompt given to ChatGPT was \"Give me common letter combinations in French\" and then cherry picking the results I thought would work best.\n","\n","If nothing is found, Latin is returned as it is the most common etymological root for english. Also note that I have decided to merge \"French\", \"Middle French\" and \"Old French\" into just \"French\" as they are too similar to tell apart. I have also done this with \"Middle English\" and \"Old English\". This leaves us with seven classes to predict."]},{"cell_type":"code","execution_count":89,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1702918294057,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"bgycpkekmof_"},"outputs":[],"source":["def ept_classifier_rule(word):\n","    if \"ua\" in word or \"eca\" in word or \"us\" in word or \"um\" in word or \"ia\" in word or \"ex\" in word or \"lib\" in word or \"oc\" in word or \"au\" in word:\n","        return(\"Latin\")\n","    elif \"ia\" in word or \"ee\" in word or \"oy\" in word or \"ow\" in word or \"ev\" in word or \"ng\" in word or \"ch\" in word or \"th\" in word or \"ph\" in word:\n","        return(\"Ancient Greek\")\n","    elif \"gh\" in word or \"kn\" in word or \"wr\" in word or \"ghth\" in word or \"tion\" in word or \"ou\" in word:\n","        return(\"English\")\n","    elif \"au\" in word or \"eau\" in word or \"ai\" in word or \"ei\" in word or \"oi\" in word or \"eu\" in word or \"gn\" in word or \"ill\" in word:\n","        return(\"French\")\n","    elif \"ka\" in word or \"ki\" in word or \"ku\" in word or \"ke\" in word or \"ko\" in word:\n","        return(\"Japanese\")\n","    elif \"ce\" in word or \"ge\" in word or \"sce\" in word or \"gh\" in word or \"gli\" in word or \"io\" in word or \"za\" in word:\n","        return(\"Italian\")\n","    elif \"ll\" in word or \"qu\" in word or \"gu\" in word or \"rr\" in word or \"gui\" in word or \"ves\" in word:\n","        return(\"Spanish\")\n","    else:\n","        return(\"Latin\")"]},{"cell_type":"code","execution_count":90,"metadata":{"executionInfo":{"elapsed":1003,"status":"ok","timestamp":1702918295046,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"ECksklT9rDS-"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","train_df, test_df = train_test_split(df4, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":91,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":330,"status":"ok","timestamp":1702918295374,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"rDHr-rf-rfzt","outputId":"81849078-d4d5-4f7d-c45b-cf5b1231104f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.3175089331291475\n"]}],"source":["test_df\n","\n","results = []\n","for i, v in test_df.iterrows():\n","  if ept_classifier_rule(v[0]) in v[2]:\n","    results.append(1)\n","  else:\n","    results.append(0)\n","\n","accuracy = sum(results)/len(results)\n","print(\"Accuracy: \" + str(accuracy))"]},{"cell_type":"code","execution_count":92,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702918295375,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"PZJsSMLArkPW","outputId":"5bc0b135-facf-43a5-ed7f-5aa80db95ce4"},"outputs":[{"name":"stdout","output_type":"stream","text":["The etymology of the word democracy is Latin\n"]}],"source":["word = \"democracy\"\n","\n","print(\"The etymology of the word \" + word + \" is \"  + ept_classifier_rule(word))"]},{"cell_type":"markdown","metadata":{"id":"-7tTsIWp1l6J"},"source":["Testing random 30"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":657,"status":"ok","timestamp":1702918300308,"user":{"displayName":"Stan Goodwin","userId":"07603469983824887396"},"user_tz":0},"id":"3kIrEC9T1lLs","outputId":"acdcf8ce-aeed-4609-b87d-fadc8c60aff1"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.36666666666666664\n"]}],"source":["test = pd.read_csv('../../data/test.csv')\n","\n","correct = []\n","all = []\n","\n","for i, r in test.iterrows():\n","  if ept_classifier_rule(r['0']) == r['2']:\n","    correct.append(1)\n","  all.append(1)\n","\n","accuracy = sum(correct)/sum(all)\n","print(accuracy)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPkkgrlXkrDf2jWJqgpgjR1","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
